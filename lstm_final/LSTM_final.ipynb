{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM_final","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyP2LbhFbwDoNWeXSr62hwCG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yReh6mzijd3U","colab_type":"text"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"L1ajBZAMyQSm","colab_type":"text"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"qHNS9vW3yTNv","colab_type":"text"},"source":["1) Ensure that the reviews1_cleaned.txt and reviews5_cleaned.txt are in the datafiles folder. <br>\n","2) Code needs to be ran on tensorflow version 1 <br>\n","3) Code has to be ran on colab with 25gb ram (>12 gb ram will be used) and runtime has to be either GPU or TPU."]},{"cell_type":"markdown","metadata":{"id":"K6j6NNj7oOkw","colab_type":"text"},"source":["# Training done\n","1) Train on food reviews (1 star) <br>\n","2) Train on food reviews (5 stars)"]},{"cell_type":"markdown","metadata":{"id":"FrMt-Ln6zSiJ","colab_type":"text"},"source":["## General Instructions\n","1) Some of the parameters will require manual changing of values if values other than the best parameter values are to be used. The lines of code to change will be denoted by comments. <br>\n","2) In addition, 3 sets of code, catering to 1 layer, 2 layers and 3 layers of LSTM are provided below, so please take note not to run all cells at once. <br>\n","3) Training epochs in the code are set as 1 to allow us to control our training and test our text generation at each epoch easily. However, it can be varied depending on the need but it is recommended to keep training epochs to be below 3 to avoid having an incomplete training epoch just before the colab instance expires. <br> <br>\n","\n","If everything is left unchanged, the notebook will be training LSTM models on the following hyperparameters by default: <br>\n","1) 1 star reviews <br>\n","2) 10 words input <br>\n","3) 100 nodes per layer <br>\n","\n","## Text Generation Instructions\n","It is possible to jump straight to the text generation using the tokenizers and model weights that we had provided. Follow the steps below. <br>\n","1) Mount the drive and change directory to the lstm_final folder <br>\n","2) Import the libraries <br>\n","3) Proceed to Text Generation section and run the code in the 3 cells\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O7Jp3rOt3HUq","colab_type":"text"},"source":["# Summary of findings\n","1) Validation loss decreases when we increase the number of LSTM layers, increase the number of nodes per LSTM layer and also increase the number of words used as input. <br>\n","2) Our best performing model is able to generate slightly complex reviews but at the expense of being unable to learn proper uppercasing because using text that have been converted to lowercases for training offers us the ability to generate more realistic text. <br>\n"]},{"cell_type":"code","metadata":{"id":"_WpajHAdIKjn","colab_type":"code","outputId":"ffb4b7a1-9f63-43c2-ceab-f26a24506841","executionInfo":{"status":"ok","timestamp":1586933049662,"user_tz":-480,"elapsed":17868,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-cgcfEgLIQqg","colab_type":"code","outputId":"bb269c7f-1f14-45fb-952e-27c673043e50","executionInfo":{"status":"ok","timestamp":1586933066131,"user_tz":-480,"elapsed":2695,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Change to the appropriate directory\n","cd 'drive/My Drive/BT4222/lstm_final'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/BT4222/lstm_final\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E2tr-cPnISNc","colab_type":"code","outputId":"ec67ad58-bd96-420d-8558-252a4907e7df","executionInfo":{"status":"ok","timestamp":1586933072077,"user_tz":-480,"elapsed":8631,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import random\n","import sys\n","import numpy as np\n","import pandas as pd\n","from pickle import dump, load\n","\n","%tensorflow_version 1.x\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.models import load_model\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras.utils import np_utils\n","from keras.utils import to_categorical\n","from keras import optimizers\n","import keras\n","from keras import layers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"AenyF21cJs9C","colab_type":"text"},"source":["### Performing additional step of data cleaning to remove the 1st and last token of text files"]},{"cell_type":"code","metadata":{"id":"IdsAsyVKITmT","colab_type":"code","colab":{}},"source":["# 1 star review\n","one_star = './datafiles/reviews1_cleaned.txt'\n","five_star = './datafiles/reviews5_cleaned.txt'\n","\n","# change the one_star to five_star if planning to train on 5 star reviews\n","text = open(one_star).read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVmWz-QgIWaf","colab_type":"code","outputId":"2244b28d-5bf6-4a52-e155-b86520b693a2","executionInfo":{"status":"ok","timestamp":1586933074930,"user_tz":-480,"elapsed":11471,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Remove the leading 'text\\n' token because of text file.\n","words_list = text.split(\" \")\n","words_list[0:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['text\\n\"I', 'went', 'at', '230', 'on', 'a', 'Monday', '.', 'It', 'was']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"DfLAnEncIW5w","colab_type":"code","outputId":"65c73327-ff59-456f-93b6-c9722fcdf392","executionInfo":{"status":"ok","timestamp":1586933074930,"user_tz":-480,"elapsed":11460,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["words_list[0] = words_list[0].split('\\n\"')[1]\n","words_list[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"zMEZSpaEIYJE","colab_type":"code","outputId":"f6b7d3c4-4919-4198-8dc0-92e7a023367d","executionInfo":{"status":"ok","timestamp":1586933074931,"user_tz":-480,"elapsed":11450,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Remove the ending '\\n' token because of text file\n","words_list[-1] = words_list[-1].split(\"\\\"\\n\")[0]\n","words_list[-1]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'!'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"dugOoiMRJ8mi","colab_type":"text"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"id":"EH8XTApyIbgf","colab_type":"code","colab":{}},"source":["# Made reference to this github \n","# https://github.com/irdanish11/Sentence-Prediction-using-LSTMs_aka-Language-Modeling/blob/master/model.py\n","# Preparing sequence of n len. In this case n = 11 because it will be split into 10 words and 1 word eventually and 10 words input is currently offering us the best performance.\n","\n","# Vary the train_len to control the length of each text sequences to be used for training.\n","train_len = 11"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWmZm0TXMra6","colab_type":"code","outputId":"b8682314-8179-4ab1-fd72-255999e79654","executionInfo":{"status":"ok","timestamp":1586933108340,"user_tz":-480,"elapsed":44847,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["text_sequences = []\n","for i in range(train_len,len(words_list)):\n","    seq = words_list[i-train_len:i]\n","    text_sequences.append(seq)\n","\n","print(text_sequences[0:10])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[['I', 'went', 'at', '230', 'on', 'a', 'Monday', '.', 'It', 'was', 'dimsum'], ['went', 'at', '230', 'on', 'a', 'Monday', '.', 'It', 'was', 'dimsum', 'I'], ['at', '230', 'on', 'a', 'Monday', '.', 'It', 'was', 'dimsum', 'I', 'hated'], ['230', 'on', 'a', 'Monday', '.', 'It', 'was', 'dimsum', 'I', 'hated', 'every'], ['on', 'a', 'Monday', '.', 'It', 'was', 'dimsum', 'I', 'hated', 'every', 'second'], ['a', 'Monday', '.', 'It', 'was', 'dimsum', 'I', 'hated', 'every', 'second', 'I'], ['Monday', '.', 'It', 'was', 'dimsum', 'I', 'hated', 'every', 'second', 'I', 'was'], ['.', 'It', 'was', 'dimsum', 'I', 'hated', 'every', 'second', 'I', 'was', 'there'], ['It', 'was', 'dimsum', 'I', 'hated', 'every', 'second', 'I', 'was', 'there', ','], ['was', 'dimsum', 'I', 'hated', 'every', 'second', 'I', 'was', 'there', ',', 'the']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5geANz3cIkGa","colab_type":"code","outputId":"212cb20f-b42b-47be-a4d6-53600567fe68","executionInfo":{"status":"ok","timestamp":1586933108341,"user_tz":-480,"elapsed":44837,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Check the total number of sequences\n","len(text_sequences)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22934217"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"nEDTYL0qIrxz","colab_type":"code","colab":{}},"source":["# Create Tokenizer, change convert_lower to False if you do not want to convert text to lowercases\n","#convert_lower = True\n","#tok = Tokenizer(filters = '', lower = convert_lower)\n","#tok.fit_on_texts(text_sequences)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zglPpIqZwHGb","colab_type":"code","colab":{}},"source":["# Save tokenizer\n","#dump(tok,open('./tokenizer/tokenizer_1_star','wb'))       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_XWZitBwHg2","colab_type":"code","colab":{}},"source":["# For subsequent run to ensure we always use the same tokenizer for transformation and training\n","\n","# tok = load(open(\"./tokenizer/tokenizer_1_star_not_lowercase\", \"rb\" ))  # Without conversion to lowercase\n","tok = load(open(\"./tokenizer/tokenizer_1_star\", \"rb\" )) # With conversion to lowercase, which has better performance"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH4L5oqRIsZx","colab_type":"code","outputId":"fa9bfa1f-90d2-45e7-be59-d26f6bf373b8","executionInfo":{"status":"ok","timestamp":1586933108342,"user_tz":-480,"elapsed":44820,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Check number of unique vocabulary\n","vocab_size = len(tok.word_counts)\n","vocab_size"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["158467"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"GYlWVbBTIvic","colab_type":"code","colab":{}},"source":["sequences = tok.texts_to_sequences(text_sequences) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcnwNePFIxk8","colab_type":"code","colab":{}},"source":["n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n","for i in range(len(sequences)):\n","    n_sequences[i] = sequences[i]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNdIUbUsI1BU","colab_type":"code","outputId":"91ef76d1-83da-4374-94fa-0c65c951e5ef","executionInfo":{"status":"ok","timestamp":1586933267914,"user_tz":-480,"elapsed":204374,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# E.g sequence has n words. n-1 words used for training, 1 word used as target.\n","train_inputs = n_sequences[:,:-1] \n","train_targets = n_sequences[:,-1] \n","seq_len = train_inputs.shape[1]\n","\n","print(train_inputs.shape)\n","print(train_targets.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(22934217, 10)\n","(22934217,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cosjECLnKH_y","colab_type":"text"},"source":["# LSTM Models\n"]},{"cell_type":"markdown","metadata":{"id":"Jc00jz7CYFOl","colab_type":"text"},"source":["The code below are segmented into 1 layer LSTM, 2 layers LSTM and 3 layers LSTM. Run only the specific segment of the code that you need."]},{"cell_type":"code","metadata":{"id":"fI_Qbf4DLZ2-","colab_type":"code","colab":{}},"source":["# Must be ran regardless of which number of layers you choose to train. \n","num_nodes = 100"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S8qmCHCaKe-P","colab_type":"text"},"source":["## 1 Layer LSTM\n"]},{"cell_type":"code","metadata":{"id":"fBxnymsCKLPX","colab_type":"code","outputId":"00f8eefb-a15c-478b-dc77-d1fee1d51b98","executionInfo":{"status":"ok","timestamp":1586933379277,"user_tz":-480,"elapsed":1219,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":322}},"source":["model_1_layer = Sequential() \n","model_1_layer.add(Embedding(vocab_size, seq_len, input_length = seq_len))\n","model_1_layer.add(LSTM(num_nodes))\n","model_1_layer.add(Dense(vocab_size + 1, activation = 'softmax'))\n","\n","# Sparse categorical crossentropy is used due to memory limitations \n","model_1_layer.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizers.adam(lr = 0.001), metrics=['accuracy'])\n","model_1_layer.summary()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 10, 10)            1584670   \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100)               44400     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 158468)            16005268  \n","=================================================================\n","Total params: 17,634,338\n","Trainable params: 17,634,338\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pqjb1-lwKydc","colab_type":"code","colab":{}},"source":["# Training done 1 epochs at a time due to the long training time need for each epochs.\n","path_name = 'lstm_model_1_layer_{}_nodes_{}_words.h5'.format(num_nodes, train_len - 1)\n","path = './checkpoints/' + path_name\n","checkpoint = ModelCheckpoint(path, monitor = 'loss', verbose=1, save_best_only = True, mode = 'min')\n","\n","print('---Training parameters---')\n","print('Number of layers: 1')\n","print('Number of nodes: ', num_nodes)\n","print('Number of words as input: ', train_len - 1)\n","print()\n","\n","# Change the epoch value to suit your needs\n","model_1_layer.fit(train_inputs, train_targets, batch_size = 1024, epochs = 3, verbose = 1, callbacks = [checkpoint])\n","model_1_layer.save(path_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8atLX5aLnBT","colab_type":"text"},"source":["## 2 Layers LSTM"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"awuXLCF2Lr65","outputId":"3144de88-0fdf-4bd2-bb27-4962fdd537a0","executionInfo":{"status":"ok","timestamp":1586930234946,"user_tz":-480,"elapsed":1303,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["model_2_layer = Sequential() \n","model_2_layer.add(Embedding(vocab_size, seq_len, input_length = seq_len))\n","model_2_layer.add(LSTM(num_nodes, return_sequences = True))\n","model_2_layer.add(LSTM(num_nodes))\n","model_2_layer.add(Dense(vocab_size + 1, activation = 'softmax'))\n","\n","# Sparse categorical crossentropy is used due to memory limitations \n","model_2_layer.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizers.adam(lr = 0.001), metrics=['accuracy'])\n","model_2_layer.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 10, 10)            1584670   \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 10, 100)           44400     \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 100)               80400     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 158468)            16005268  \n","=================================================================\n","Total params: 17,714,738\n","Trainable params: 17,714,738\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r1p9Ky-lLr67","colab":{}},"source":["# Training done 1 epochs at a time due to the long training time need for each epochs.\n","path_name = 'lstm_model_2_layer_{}_nodes_{}_words.h5'.format(num_nodes, train_len - 1)\n","path = './checkpoints/' + path_name\n","checkpoint = ModelCheckpoint(path, monitor = 'loss', verbose=1, save_best_only = True, mode = 'min')\n","\n","print('---Training parameters---')\n","print('Number of layers: 2')\n","print('Number of nodes: ', num_nodes)\n","print('Number of words as input: ', train_len - 1)\n","print()\n","\n","# Change the epoch value to suit your needs\n","model_2_layer.fit(train_inputs, train_targets, batch_size = 1024, epochs = 1, verbose = 1, callbacks = [checkpoint])\n","model_2_layer.save(path_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vdylEbXLzNT","colab_type":"text"},"source":["## 3 Layers LSTM"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"339ZZ2hxLuHX","outputId":"0691108c-8b7f-412a-9761-174d55cd6f46","executionInfo":{"status":"ok","timestamp":1586930462045,"user_tz":-480,"elapsed":1440,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":319}},"source":["model_3_layer = Sequential() \n","model_3_layer.add(Embedding(vocab_size, seq_len, input_length = seq_len))\n","model_3_layer.add(LSTM(num_nodes, return_sequences = True))\n","model_3_layer.add(LSTM(num_nodes, return_sequences = True))\n","model_3_layer.add(LSTM(num_nodes))\n","model_3_layer.add(Dense(vocab_size + 1, activation = 'softmax'))\n","\n","# Sparse categorical crossentropy is used due to memory limitations \n","model_3_layer.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizers.adam(lr = 0.001), metrics=['accuracy'])\n","model_3_layer.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 10, 10)            1584670   \n","_________________________________________________________________\n","lstm_7 (LSTM)                (None, 10, 50)            12200     \n","_________________________________________________________________\n","lstm_8 (LSTM)                (None, 10, 50)            20200     \n","_________________________________________________________________\n","lstm_9 (LSTM)                (None, 50)                20200     \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 158468)            8081868   \n","=================================================================\n","Total params: 9,719,138\n","Trainable params: 9,719,138\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"w-c9pYfMLuHZ","colab":{}},"source":["# Training done 1 epochs at a time due to the long training time need for each epochs.\n","path_name = 'lstm_model_3_layer_{}_nodes_{}_words.h5'.format(num_nodes, train_len - 1)\n","path = './checkpoints/' + path_name\n","checkpoint = ModelCheckpoint(path, monitor = 'loss', verbose=1, save_best_only = True, mode = 'min')\n","\n","print('---Training parameters---')\n","print('Number of layers: 3')\n","print('Number of nodes: ', num_nodes)\n","print('Number of words as input: ', train_len - 1)\n","print()\n","\n","# Change the epoch value to suit your needs\n","model_3_layer.fit(train_inputs, train_targets, batch_size = 1024, epochs = 1, verbose = 1, callbacks = [checkpoint])\n","model_3_layer.save(path_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ea2U1qUFL2H-","colab_type":"text"},"source":["# Text Generation \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"CzRAal8uL4u4","colab_type":"code","colab":{}},"source":["''' \n","Generate text\n","\n","Input: 1) model \n","       2) tokenizer\n","       3) input_text: Provide a prompt in str format\n","       4) num_gen_words: number of words to generate. \n","'''\n","def gen_text(model, tokenizer, input_text, num_gen_words):\n","    seq_len = model.input_shape[1]\n","    output_text = [input_text]\n","    pred_word = None\n","    final_text = ''\n","\n","    for i in range(num_gen_words):\n","        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n","        if(pred_word == ''):\n","          encoded_text.append(9) \n","        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len,truncating='pre')\n","        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n","\n","        pred_word = tokenizer.index_word[pred_word_ind]\n","        input_text += ' ' + pred_word\n","        output_text.append(pred_word)\n","\n","    for text in output_text:\n","      if text not in ['!', '?', ',', '.', '']:\n","        final_text = final_text + ' '+ text\n","      else:\n","        final_text = final_text + text\n","\n","    return final_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UM9WUg0qMH5U","colab_type":"code","colab":{}},"source":["'''\n","Sample loading of best model.\n","\n","We have provided 3 model weights that we had train for this project in the checkpoints folder. \n","1) Weights of best performing model for 1 star reviews with conversion to lowercases: 'model_1_star_best'\n","2) Weights of best performing model for 5 star reviews with conversion to lowercases: 'model_5_star_best'\n","3) Weights of model for 1 star reviews without conversion to lowercases: 'model_1_star_not_lowercase'\n","\n","The weights for the model that we had trained on the 1 star review without conversion to lowercases is also provided for you to compare the performances.\n","\n","The weights variable needs to be changed if training is done on your part. The weights for the additional trainings will also be stored in the checkpoints folder.\n","\n","The tokenizers can be found in the tokenizers folder.\n","1) tokenizer_1_star: For 1 star reviews\n","2) tokenizer_5_star: For 5 star reviews\n","3) tokenizer_1_star_not_lowercase: For 1 star reviews without conversion to lowercases\n","'''\n","weights = 'model_1_star_best.h5'\n","pathname = './checkpoints/' + weights\n","best_model = load_model(pathname)\n","tok = load(open(\"./tokenizer/tokenizer_1_star\", \"rb\" )) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"09lfi5dZMEMY","colab_type":"code","outputId":"7d9506bf-7325-4533-8e24-af4506e42521","executionInfo":{"status":"ok","timestamp":1586932585552,"user_tz":-480,"elapsed":7873,"user":{"displayName":"Adrian Tan","photoUrl":"https://lh5.googleusercontent.com/-_KoHbdGGfwM/AAAAAAAAAAI/AAAAAAAAAPU/5j2-dmEXssI/s64/photo.jpg","userId":"10191054836778583419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Sample text generation \n","gen_text(best_model, tok, 'I am disappointed', 30)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' I am disappointed. i was so excited to try this place again. i ordered a chicken sandwich and it was a little bit of a jar. the'"]},"metadata":{"tags":[]},"execution_count":12}]}]}